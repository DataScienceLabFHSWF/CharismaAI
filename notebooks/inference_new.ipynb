{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c36151c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-06 14:26:50.007423: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-06 14:26:50.007479: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-06 14:26:50.008627: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-06 14:26:50.016027: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-06 14:26:50.761954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/datasciencefhswf/anaconda3/envs/charisma/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cce2837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../test_speeches/Coding_Differneces_103.xlsx', '../test_speeches/Coding_Differneces_11.xlsx', '../test_speeches/Coding_Differneces_110.xlsx', '../test_speeches/Coding_Differneces_118.xlsx', '../test_speeches/Coding_Differneces_12.xlsx', '../test_speeches/Coding_Differneces_13.xlsx', '../test_speeches/Coding_Differneces_134.xlsx', '../test_speeches/Coding_Differneces_140.xlsx', '../test_speeches/Coding_Differneces_150.xlsx', '../test_speeches/Coding_Differneces_4.xlsx', '../test_speeches/Coding_Differneces_5.xlsx', '../test_speeches/Speech_10.xlsx', '../test_speeches/Speech_1_Obama_.xlsx', '../test_speeches/Speech_2_Maggie_Thatcher.xlsx', '../test_speeches/Speech_3_Testspeech.xlsx', '../test_speeches/Speech_4.xlsx', '../test_speeches/Speech_5.xlsx', '../test_speeches/Speech_6.xlsx', '../test_speeches/Speech_7.xlsx', '../test_speeches/Speech_8.xlsx', '../test_speeches/Speech_9.xlsx']\n"
     ]
    }
   ],
   "source": [
    "path = '../test_speeches/'\n",
    "files = sorted(glob(path+'*.xlsx'))\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22761736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "charisma = torch.load('../../CharismaModels/CharismaBERT100_new_data_mergedmetaphors.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35b2289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66af6fb0644f407aa86711aec7ada705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ca982ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(test_data):    \n",
    "    label=[]\n",
    "    for i in range(len(test_data)):\n",
    "        if any(test_data.iloc[i]==1):\n",
    "            label.append(1)\n",
    "        else:\n",
    "            label.append(0)\n",
    "    test_data[\"label\"]=label\n",
    "    #print('Number of test sentences: {:,}\\n'.format(test_data.shape[0]))\n",
    "# Create sentence and label lists\n",
    "    sentences = test_data.Sentence.values\n",
    "    labels = test_data.drop(columns=['Sentence'])['label'].values\n",
    "    MAX_LEN = 126\n",
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "# For every sentence...\n",
    "    for sent in sentences:\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "        encoded_sent = tokenizer.encode(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                   )\n",
    "    \n",
    "        input_ids.append(encoded_sent)\n",
    "# Pad our input tokens\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, \n",
    "                          dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    labels = torch.tensor(labels,dtype=torch.long)\n",
    "# Create attention masks\n",
    "    attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask) \n",
    "# Convert to tensors.\n",
    "    prediction_inputs = torch.tensor(input_ids)\n",
    "    prediction_masks = torch.tensor(attention_masks)\n",
    "    prediction_labels = torch.tensor(labels)\n",
    "# Set the batch size.  \n",
    "    batch_size = 32  \n",
    "# Create the DataLoader.\n",
    "    prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
    "    return  prediction_inputs, prediction_masks, prediction_labels,  prediction_data, prediction_sampler, prediction_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d66c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, test_data):\n",
    "    prediction_inputs, prediction_masks, prediction_labels, prediction_data, prediction_sampler, prediction_dataloader = preprocess_data(test_data)\n",
    "    # Prediction on test set\n",
    "    #print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    # Tracking variables \n",
    "    predictions , true_labels, prediction_probs = [], [],[] #Atefeh\n",
    "    # Predict \n",
    "    for batch in prediction_dataloader:\n",
    "      # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "      # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "      # Telling the model not to compute or store gradients, saving memory and \n",
    "      # speeding up prediction\n",
    "        with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "            outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "  # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.detach().to('cpu').numpy()\n",
    "  \n",
    "        prediction_probs.append (tf.nn.softmax(logits))\n",
    "\n",
    "  # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    \n",
    "    print('DONE.')\n",
    "    \n",
    "    \n",
    "    for i in range(len(predictions)):  \n",
    "      # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "      # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "      # in to a list of 0s and 1s.\n",
    "        pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "    \n",
    "    flat_predictions = [item for sublist in prediction_probs for item in sublist]\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "    model.cpu()\n",
    "    return flat_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e560829a-3cad-473d-80ad-718a8256b65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coding_Differneces_103'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0][17:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52715730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Sentence', 'Metaphor/Simile', 'Rhetorical questions',\n",
      "       'Stories / anecdotes', 'Contrasts', 'Lists / Repetition ',\n",
      "       'Moral conviction', 'Sentiment of the collective',\n",
      "       'Ambitious goals / Setting high expectations', 'Confidence in goals'],\n",
      "      dtype='object')\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_103_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_11_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_110_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_118_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_12_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_13_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_134_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_140_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_150_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_4_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Coding_Differneces_5_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Speech_10_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Speech_1_Obama__AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Speech_2_Maggie_Thatcher_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Speech_3_Testspeech_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Speech_4_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Speech_5_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Speech_6_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Speech_7_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Speech_8_AI.xlsx\n",
      "DONE.\n",
      "File written to: ../test_speeches/predictions/Speech_9_AI.xlsx\n"
     ]
    }
   ],
   "source": [
    "cols = pd.read_excel(files[0], skiprows=6).drop(columns=['Unnamed: 0']).columns\n",
    "print(cols)\n",
    "for file in files:\n",
    "    data = pd.read_excel(file, skiprows=6).drop(columns=['Unnamed: 0']).fillna('')\n",
    "    charisma_predictions = make_prediction(charisma, data)\n",
    "    data.label = charisma_predictions\n",
    "\n",
    "    \n",
    "    final_df = data.drop(columns='label').drop(0) \n",
    "    final_df.to_excel('../test_speeches/predictions/'+file[17:-5]+'_AI.xlsx')\n",
    "    print('File written to: ' + '../test_speeches/predictions/'+file[17:-5]+'_AI.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d79936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fb400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
